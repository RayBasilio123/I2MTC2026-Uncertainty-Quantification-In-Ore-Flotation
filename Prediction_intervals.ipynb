{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_GArG996vjS",
        "outputId": "84ab7075-fb44-4809-aa0a-d9f8763355df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ngboost\n",
            "  Downloading ngboost-0.5.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting lifelines>=0.25 (from ngboost)\n",
            "  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from ngboost) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.6 in /usr/local/lib/python3.12/dist-packages (from ngboost) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.7.2 in /usr/local/lib/python3.12/dist-packages (from ngboost) (1.16.2)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.12/dist-packages (from ngboost) (4.67.1)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.12/dist-packages (from lifelines>=0.25->ngboost) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.12/dist-packages (from lifelines>=0.25->ngboost) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.12/dist-packages (from lifelines>=0.25->ngboost) (1.8.0)\n",
            "Collecting autograd-gamma>=0.3 (from lifelines>=0.25->ngboost)\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting formulaic>=0.2.2 (from lifelines>=0.25->ngboost)\n",
            "  Downloading formulaic-1.2.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0,>=1.6->ngboost) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0,>=1.6->ngboost) (3.6.0)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines>=0.25->ngboost)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: narwhals>=1.17 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.2.2->lifelines>=0.25->ngboost) (1.17.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0->lifelines>=0.25->ngboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1->lifelines>=0.25->ngboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1->lifelines>=0.25->ngboost) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines>=0.25->ngboost) (1.17.0)\n",
            "Downloading ngboost-0.5.6-py3-none-any.whl (35 kB)\n",
            "Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.2.1-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: autograd-gamma\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=81e7d317855916f118ad9cea81383e7281588ae746a6695b606d8ea6f51d7e7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/37/21/0a719b9d89c635e89ff24bd93b862882ad675279552013b2fb\n",
            "Successfully built autograd-gamma\n",
            "Installing collected packages: interface-meta, autograd-gamma, formulaic, lifelines, ngboost\n",
            "Successfully installed autograd-gamma-0.5.0 formulaic-1.2.1 interface-meta-1.3.0 lifelines-0.30.0 ngboost-0.5.6\n",
            "Requirement already satisfied: xgboost>=2.0.0 in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost>=2.0.0) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost>=2.0.0) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost>=2.0.0) (1.16.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install ngboost\n",
        "%pip install \"xgboost>=2.0.0\"\n",
        "try:\n",
        "    from ngboost import NGBRegressor\n",
        "    from ngboost.distns import Normal\n",
        "    HAVE_NGBOOST = True\n",
        "except Exception:\n",
        "    HAVE_NGBOOST = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Fq30Gw5sYCve"
      },
      "outputs": [],
      "source": [
        "# agrega_resultados_por_features_com_logs.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JJ1v-8Ds6BYz"
      },
      "outputs": [],
      "source": [
        "# ====== Config simples de log ======\n",
        "VERBOSE = True          # imprime tudo\n",
        "PRINT_EVERY = 1         # imprimir a cada N runs (1 = toda run)\n",
        "N_RUNS = 20\n",
        "SEED_BASE = 42          # semente base para reprodutibilidade do fluxo de sementes\n",
        "\n",
        "# ====== Dados ======\n",
        "df = pd.read_csv('dados_2h_aggregado_final.csv')\n",
        "df['Data'] = pd.to_datetime(df['Data'])\n",
        "\n",
        "features1 = ['TO_AI6401_01', 'TO_AI6402_01', 'TO_BO6311_EST1', 'TO_DI6311_01', 'TO_FI6311_01',\n",
        "             'TO_FI8271_01', 'TO_FY6104_01', 'TO_LI6401_01', 'TO_LI6401_02', 'TO_LI6402_01',\n",
        "             'TO_LI6411_01', 'TO_LI6411_02', 'TO_LI6412_01', 'TO_LI6412_02', 'TO_LI6431_01',\n",
        "             'TO_LI6432_01', 'TO_MF6401_M1_EST1', 'TO_MF6402_M1_EST1', 'TO_MF6411_M1_EST1',\n",
        "             'TO_MF6412_M1_EST1', 'TO_MF6421_M1_EST1', 'TO_PHY4804_02', 'TO_PHY4814_02', 'TO_SIMF640101',\n",
        "             'TO_SIMF640102', 'TO_SIMF640201', 'TO_SIMF640202', 'TO_SIMF641101', 'TO_SIMF641102',\n",
        "             'TO_SIMF641201', 'TO_SIMF641202', 'TO_SIMF6421', 'TO_SIMF6422', 'TO_SIMF6431', 'TO_SIMF6432', 'TO_WI4804_01']\n",
        "features2 = ['TO_WI4804_01', 'TO_PHY4814_02', 'TO_PHY4804_02', 'TO_SIMF641201', 'TO_LI6412_01', 'TO_SIMF640202', 'TO_MF6401_M1_EST1', 'TO_SIMF6422', 'TO_SIMF641102', 'TO_FI8271_01']\n",
        "features3 = ['TO_MF6412_M1_EST1', 'TO_FI8271_01', 'TO_PHY4814_02', 'TO_WI4804_01', 'TO_SIMF6421', 'TO_FI6311_01', 'TO_PHY4804_02', 'TO_SIMF6431', 'TO_SIMF640202', 'TO_LI6411_01']\n",
        "features4 = ['TO_PHY4804_02', 'TO_AI6401_01', 'TO_AI6402_01', 'TO_LI6412_01', 'TO_LI6412_02',\n",
        "             'TO_LI6431_01', 'TO_FI8271_01', 'TO_PHY4814_02', 'TO_FY6104_01', 'TO_WI4804_01',\n",
        "             'TO_SIMF640101', 'TO_SIMF640201', 'TO_SIMF640202', 'TO_SIMF641101', 'TO_SIMF641102',\n",
        "             'TO_SIMF641201', 'TO_SIMF641202', 'TO_SIMF6421', 'TO_SIMF6422', 'TO_SIMF6431']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l3kqNqETn4C",
        "outputId": "62bd48ff-6423-4297-cac5-743e677d638c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting Prediction_intervals.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile Prediction_intervals.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Prediction_intervals.py\n",
        "------------------\n",
        "Script para construção de intervalos de predição com múltiplos métodos.\n",
        "\n",
        "Uso (variáveis de ambiente):\n",
        "- DATA_FILE: caminho do CSV (default: dados_2h_aggregado_final.csv)\n",
        "- SPLIT_TYPE: \"random\" | \"temporal\"\n",
        "- MODEL_NAME: \"xgb\" | \"lgbm\" | \"hgb\" (default: xgb)\n",
        "- INTERVAL_METHOD: ver lista em METHOD_CHOICES\n",
        "- ALPHA: nível (default: 0.05)\n",
        "- SEED: semente\n",
        "- USE_ZSCORE_OUTLIERS: 0/1\n",
        "- ZS_THRESH: 3.0\n",
        "- CONSERVATIVE_Q: 0/1 (default: 1)\n",
        "- USE_POINT_MODEL: 0/1\n",
        "\"\"\"\n",
        "import os, math, random, inspect\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.base import clone\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\n",
        "\n",
        "# backends\n",
        "HAVE_XGB = True\n",
        "try:\n",
        "    from xgboost import XGBRegressor\n",
        "except Exception:\n",
        "    HAVE_XGB = False\n",
        "\n",
        "HAVE_LGBM = True\n",
        "try:\n",
        "    from lightgbm import LGBMRegressor\n",
        "except Exception:\n",
        "    HAVE_LGBM = False\n",
        "\n",
        "# NGBoost\n",
        "HAVE_NGB = True\n",
        "try:\n",
        "    from ngboost import NGBRegressor\n",
        "    from ngboost.distns import Normal\n",
        "except Exception:\n",
        "    HAVE_NGB = False\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "CSV_FILE   = os.environ.get(\"DATA_FILE\", \"dados_2h_aggregado_final.csv\")\n",
        "SPLIT_TYPE = os.environ.get(\"SPLIT_TYPE\", \"random\").lower()          # 'random' | 'temporal'\n",
        "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"xgb\").lower()             # 'xgb' | 'lgbm' | 'hgb'\n",
        "METHOD     = os.environ.get(\"INTERVAL_METHOD\", \"enbpi_rf\").lower()\n",
        "ALPHA      = float(os.environ.get(\"ALPHA\", \"0.05\"))\n",
        "SEED       = int(os.environ.get(\"SEED\",  \"42\"))\n",
        "\n",
        "USE_ZSCORE_OUTLIERS = bool(int(os.environ.get(\"USE_ZSCORE_OUTLIERS\", \"0\")))  # 1/0\n",
        "ZS_THRESH           = float(os.environ.get(\"ZS_THRESH\", \"3.0\"))\n",
        "\n",
        "# quantil conservador (garantia finita-amostral) vs linear\n",
        "CONSERVATIVE_Q = bool(int(os.environ.get(\"CONSERVATIVE_Q\", \"1\")))\n",
        "\n",
        "# por padrão NÃO usar um modelo separado para ponto (evita “apertar” métricas com cal)\n",
        "USE_SEPARATE_POINT_MODEL = bool(int(os.environ.get(\"USE_POINT_MODEL\", \"0\")))\n",
        "\n",
        "# Semente global\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "RESULTS_DIR = Path(\"results\"); RESULTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "FEATURES = ['TO_PHY4804_02', 'TO_AI6401_01', 'TO_AI6402_01', 'TO_LI6412_01', 'TO_LI6412_02',\n",
        "             'TO_LI6431_01', 'TO_FI8271_01', 'TO_PHY4814_02', 'TO_FY6104_01', 'TO_WI4804_01',\n",
        "             'TO_SIMF640101', 'TO_SIMF640201', 'TO_SIMF640202', 'TO_SIMF641101', 'TO_SIMF641102',\n",
        "             'TO_SIMF641201', 'TO_SIMF641202', 'TO_SIMF6421', 'TO_SIMF6422', 'TO_SIMF6431']\n",
        "\n",
        "TARGET = 'P_CONFLTTO_QQ_GLOBAL_SIO2'\n",
        "\n",
        "# Paleta\n",
        "PALETTE = sns.color_palette(\"pastel\")\n",
        "COLOR_REAL = PALETTE[0]; COLOR_PRED = PALETTE[1]; COLOR_FILL = PALETTE[2]\n",
        "\n",
        "# =========================\n",
        "# Utils\n",
        "# =========================\n",
        "def rmse(y_true, y_pred): return math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def q_conformal(scores: np.ndarray, alpha: float, conservative: bool = True) -> float:\n",
        "    s = np.asarray(scores).ravel()\n",
        "    n = len(s)\n",
        "    if n == 0: raise ValueError(\"scores vazios.\")\n",
        "    if conservative:\n",
        "        from math import ceil\n",
        "        k = ceil((n + 1) * (1 - alpha))\n",
        "        k = min(max(1, k), n)\n",
        "        return np.partition(s, k-1)[k-1]\n",
        "    else:\n",
        "        return np.quantile(s, 1 - alpha, method=\"linear\")\n",
        "\n",
        "def get_point_model(name: str, random_state: int = 42):\n",
        "    name = name.lower()\n",
        "    if name == \"xgb\" and HAVE_XGB:\n",
        "        # ==== seus hiperparâmetros do XGB para modelos de ponto ====\n",
        "      return XGBRegressor(\n",
        "    n_estimators=1723,\n",
        "    learning_rate=0.037956993618682176,\n",
        "    max_depth=7,\n",
        "    subsample=0.7836258304564441,\n",
        "    colsample_bytree=0.9491409614285389,\n",
        "    min_child_weight=3.7584550421359695,\n",
        "    reg_lambda=15.78535411656014,\n",
        "    reg_alpha=2.267819381076329,\n",
        "    gamma=0.3719023165905777,\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"rmse\",\n",
        "    random_state=random_state,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "    if name == \"lgbm\" and HAVE_LGBM:\n",
        "        return LGBMRegressor(\n",
        "            n_estimators=600, learning_rate=0.05,\n",
        "            num_leaves=2**12, max_depth=12, min_data_in_leaf=20,\n",
        "            subsample=0.8, colsample_bytree=0.8,\n",
        "            random_state=random_state, verbose=-1\n",
        "        )\n",
        "\n",
        "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "    return HistGradientBoostingRegressor(random_state=random_state)\n",
        "\n",
        "def ensure_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "def split_data(X: pd.DataFrame, y: pd.Series, split_type: str, seed: int,\n",
        "               train_frac=0.80, cal_frac=0.10):\n",
        "   \n",
        "    n = len(X)\n",
        "    n_train = int(n * train_frac)  \n",
        "    n_cal = int(n * cal_frac)      \n",
        "    n_test = n - n_train - n_cal   \n",
        "\n",
        "    if split_type == \"temporal\":\n",
        "        X_train = X.iloc[:n_train]; y_train = y.iloc[:n_train]\n",
        "        X_cal   = X.iloc[n_train:n_train + n_cal]; y_cal = y.iloc[n_train:n_train + n_cal]\n",
        "        X_test  = X.iloc[n_train + n_cal:]; y_test = y.iloc[n_train + n_cal:]\n",
        "        return X_train, X_cal, X_test, y_train, y_cal, y_test\n",
        "\n",
        "\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=1.0 - train_frac,\n",
        "                                               shuffle=True, random_state=seed)\n",
        "    X_tr, X_ca, y_tr, y_ca = train_test_split(X_tr, y_tr, test_size=cal_frac / (train_frac),\n",
        "                                               shuffle=True, random_state=seed + 1)\n",
        "    return X_tr, X_ca, X_te, y_tr, y_ca, y_te\n",
        "\n",
        "# =========================\n",
        "# Intervalos canônicos\n",
        "# =========================\n",
        "def t_student_intervals(y_pred, residuals, n_train, p, alpha=ALPHA):\n",
        "    \"\"\"Intervalo aproximado usando t-Student sobre resíduos de treino.\"\"\"\n",
        "    s_err = np.std(residuals, ddof=1)\n",
        "    dof = max(n_train - p, 1)\n",
        "    t_val = stats.t.ppf(1 - alpha/2, dof)\n",
        "    margin = t_val * s_err * math.sqrt(1 + 1.0/n_train)\n",
        "    return y_pred - margin, y_pred + margin, dict(s_err=s_err, dof=dof, t_val=t_val, margin=margin)\n",
        "\n",
        "def full_conformal_cvplus(model, X_train_cal, y_train_cal, X_test, alpha=ALPHA, folds=5, seed=SEED):\n",
        "    \"\"\"Full conformal com resíduos out-of-fold (CV+).\"\"\"\n",
        "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
        "    preds_oof = np.zeros(len(X_train_cal))\n",
        "    for tr_idx, va_idx in kf.split(X_train_cal):\n",
        "        m = clone(model)\n",
        "        m.fit(X_train_cal.iloc[tr_idx], y_train_cal.iloc[tr_idx])\n",
        "        preds_oof[va_idx] = m.predict(X_train_cal.iloc[va_idx])\n",
        "    scores = np.abs(y_train_cal.values - preds_oof)\n",
        "    q_hat = q_conformal(scores, alpha, conservative=CONSERVATIVE_Q)\n",
        "    m_final = clone(model); m_final.fit(X_train_cal, y_train_cal)\n",
        "    y_pred = m_final.predict(X_test)\n",
        "    return y_pred, y_pred - q_hat, y_pred + q_hat, dict(q_hat=q_hat)\n",
        "\n",
        "def full_conformal_prediction_intervalsF(model, X_train, X_test, y_train, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Variante \"F\": Full Conformal Prediction simples (sem CV+), usando o quantil\n",
        "    ((n + 1) * (1 - alpha))/n aplicado aos resíduos absolutos de treino.\n",
        "    \"\"\"\n",
        "    m = clone(model); m.fit(X_train, y_train)\n",
        "    y_pred_train = m.predict(X_train)\n",
        "    residuals_train = np.abs(y_train - y_pred_train)\n",
        "    n = len(X_train)\n",
        "    q_quantile = (n + 1) * (1 - alpha) / n\n",
        "    q_ = np.quantile(residuals_train, q_quantile, method=\"linear\")\n",
        "    y_pred_test = m.predict(X_test)\n",
        "    lower_bound = y_pred_test - q_\n",
        "    upper_bound = y_pred_test + q_\n",
        "    return y_pred_test, lower_bound, upper_bound, dict(q_hat=float(q_))\n",
        "\n",
        "def split_conformal_intervals(model, X_train, y_train, X_cal, y_cal, X_test, alpha=ALPHA):\n",
        "    \"\"\"Split conformal canônico: mesmo modelo para cal e teste.\"\"\"\n",
        "    m = clone(model)\n",
        "    m.fit(X_train, y_train)\n",
        "    y_cal_pred = m.predict(X_cal)\n",
        "    scores = np.abs(y_cal.values - y_cal_pred)\n",
        "    q_hat = q_conformal(scores, alpha, conservative=CONSERVATIVE_Q)\n",
        "    y_pred = m.predict(X_test)\n",
        "    return y_pred, y_pred - q_hat, y_pred + q_hat, dict(q_hat=q_hat)\n",
        "\n",
        "\n",
        "\n",
        "def bootstrap_residual_intervals(model, X_train, y_train, X_cal, y_cal, X_test,\n",
        "                                 n_bootstrap=1000, alpha=ALPHA, seed=SEED):\n",
        "    \"\"\"\n",
        "    Bootstrap de resíduos (percentil):\n",
        "      - Treina no TRAIN\n",
        "      - Calcula resíduos NO CAL (out-of-sample)\n",
        "      - Reamostra resíduos do CAL e os adiciona às predições de teste\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    m = clone(model)\n",
        "    m.fit(X_train, y_train)\n",
        "\n",
        "    # predições\n",
        "    y_pred_test = m.predict(X_test)\n",
        "    y_pred_cal  = m.predict(X_cal)\n",
        "\n",
        "    # resíduos OUT-OF-SAMPLE (calibração)\n",
        "    res_cal = (y_cal.values if hasattr(y_cal, \"values\") else np.asarray(y_cal)) - y_pred_cal\n",
        "\n",
        "    # recentrar resíduos (garante média ~0)\n",
        "    res_cal = res_cal - np.mean(res_cal)\n",
        "\n",
        "    n_obs = len(X_test)\n",
        "    preds = np.empty((n_bootstrap, n_obs), dtype=float)\n",
        "    for i in range(n_bootstrap):\n",
        "        boot_res = rng.choice(res_cal, size=n_obs, replace=True)\n",
        "        preds[i, :] = y_pred_test + boot_res\n",
        "\n",
        "    lo = np.percentile(preds, 100 * (alpha / 2.0), axis=0)\n",
        "    up = np.percentile(preds, 100 * (1 - alpha / 2.0), axis=0)\n",
        "    return y_pred_test, lo, up, dict(n_bootstrap=n_bootstrap, seed=seed)\n",
        "\n",
        "def bootstrap_prediction_intervalsF(model, X_train, y_train, X_test, n_bootstrap=1000, alpha=0.05, seed=SEED):\n",
        "    \"\"\"Variante 'F' do bootstrap utilizando sklearn.utils.resample.\"\"\"\n",
        "    m = clone(model); m.fit(X_train, y_train)\n",
        "    y_pred_test = m.predict(X_test)\n",
        "    residuals = y_train - m.predict(X_train)\n",
        "    n_obs = len(X_test)\n",
        "    bootstrap_predictions = np.zeros((n_bootstrap, n_obs))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    for i in range(n_bootstrap):\n",
        "        idx = rng.integers(0, len(residuals), size=n_obs)\n",
        "        bootstrap_residuals = residuals.values[idx] if hasattr(residuals, \"values\") else residuals[idx]\n",
        "        bootstrap_predictions[i, :] = y_pred_test + bootstrap_residuals\n",
        "    lower_bound = np.percentile(bootstrap_predictions, 100 * alpha / 2, axis=0)\n",
        "    upper_bound = np.percentile(bootstrap_predictions, 100 * (1 - alpha / 2), axis=0)\n",
        "    return y_pred_test, lower_bound, upper_bound, dict(n_bootstrap=n_bootstrap, seed=seed)\n",
        "\n",
        "d\n",
        "def pi_cqr(X_tr, y_tr, X_cal, y_cal, X_te, alpha=ALPHA, seed=SEED):\n",
        "    \"\"\"CQR usando GradientBoostingRegressor (quantile).\"\"\"\n",
        "    lo = GradientBoostingRegressor(loss='quantile', alpha=alpha/2, n_estimators=400, random_state=seed)\n",
        "    hi = GradientBoostingRegressor(loss='quantile', alpha=1 - alpha/2, n_estimators=400, random_state=seed)\n",
        "    lo.fit(X_tr, y_tr); ql_cal = lo.predict(X_cal)\n",
        "    hi.fit(X_tr, y_tr); qh_cal = hi.predict(X_cal)\n",
        "    scores = np.maximum(ql_cal - y_cal.values, y_cal.values - qh_cal)\n",
        "    qval = q_conformal(scores, alpha, conservative=CONSERVATIVE_Q)\n",
        "    X_tc = pd.concat([X_tr, X_cal], axis=0); y_tc = pd.concat([y_tr, y_cal], axis=0)\n",
        "    lo.fit(X_tc, y_tc); ql_te = lo.predict(X_te)\n",
        "    hi.fit(X_tc, y_tc); qh_te = hi.predict(X_te)\n",
        "    point_pred = 0.5*(ql_te + qh_te)\n",
        "    return point_pred, ql_te - qval, qh_te + qval, dict(q_hat=qval)\n",
        "\n",
        "\n",
        "def pi_enbpi(X_tr, y_tr, X_cal, y_cal, X_te, alpha=ALPHA, seed=SEED):\n",
        "    return pi_enbpiF(X_tr, y_tr, X_cal, y_cal, X_te, alpha=alpha, seed=seed)\n",
        "\n",
        "def pi_ngboostF(X_tr, y_tr, X_te, alpha=ALPHA, seed=SEED):\n",
        "    \"\"\"NGBoost (Normal). Intervalo direto via PPF do Normal previsto.\"\"\"\n",
        "    if not HAVE_NGB:\n",
        "        raise RuntimeError(\"NGBoost não disponível.\")\n",
        "    ngb = NGBRegressor(Dist=Normal, n_estimators=600, learning_rate=0.03, random_state=seed)\n",
        "    ngb.fit(X_tr, y_tr)\n",
        "    dist = ngb.pred_dist(X_te)\n",
        "    mu = dist.loc\n",
        "    lo = dist.ppf(alpha/2)\n",
        "    up = dist.ppf(1 - alpha/2)\n",
        "    return mu, lo, up, dict()\n",
        "\n",
        "def pi_ngboost(X_tr, y_tr, X_te, alpha=ALPHA, seed=SEED):\n",
        "    return pi_ngboostF(X_tr, y_tr, X_te, alpha=alpha, seed=seed)\n",
        "\n",
        "# ====\n",
        "# ===== CQR com XGBoost (quantile) e EnbPI com XGB =====\n",
        "\n",
        "# Seus hípers do XGB — separando para ponto (rmse) e para quantil\n",
        "XGB_PARAMS_POINT = dict(\n",
        "    n_estimators=1723,\n",
        "    learning_rate=0.037956993618682176,\n",
        "    max_depth=7,\n",
        "    subsample=0.7836258304564441,\n",
        "    colsample_bytree=0.9491409614285389,\n",
        "    min_child_weight=3.7584550421359695,\n",
        "    reg_lambda=15.78535411656014,\n",
        "    reg_alpha=2.267819381076329,\n",
        "    gamma=0.3719023165905777,\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"rmse\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "XGB_PARAMS_QUANTILE = dict(\n",
        "    n_estimators=1723,\n",
        "    learning_rate=0.037956993618682176,\n",
        "    max_depth=7,\n",
        "    subsample=0.7836258304564441,\n",
        "    colsample_bytree=0.9491409614285389,\n",
        "    min_child_weight=3.7584550421359695,\n",
        "    reg_lambda=15.78535411656014,\n",
        "    reg_alpha=2.267819381076329,\n",
        "    gamma=0.3719023165905777,\n",
        "    tree_method=\"hist\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "\n",
        "def pi_cqr_xgb(\n",
        "    X_tr, y_tr, X_cal, y_cal, X_te,\n",
        "    alpha=ALPHA, seed=SEED,\n",
        "    xgb_params=None, use_q50_point=True,\n",
        "    early_stopping=True, es_val_frac=0.2, es_rounds=50\n",
        "):\n",
        "    if not HAVE_XGB:\n",
        "        raise RuntimeError(\"XGBoost não disponível.\")\n",
        "    from xgboost import XGBRegressor\n",
        "\n",
        "    # defaults internos; serão sobrescritos por xgb_params\n",
        "    base_def = dict(\n",
        "        subsample=1.0, reg_lambda=1.0, reg_alpha=10.0, min_child_weight=3.0,\n",
        "        max_depth=6, learning_rate=0.01, gamma=0.0, colsample_bytree=0.6,\n",
        "        n_estimators=300, n_jobs=-1, tree_method=\"hist\", random_state=seed\n",
        "    )\n",
        "    if xgb_params:\n",
        "        base_def.update(xgb_params)\n",
        "\n",
        "    q_lo, q_hi = alpha/2.0, 1.0 - alpha/2.0\n",
        "\n",
        "    # split interno para early stopping (não usa calibração)\n",
        "    if early_stopping:\n",
        "        X_tr_in, X_val, y_tr_in, y_val = train_test_split(\n",
        "            X_tr, y_tr, test_size=es_val_frac, random_state=seed, shuffle=True\n",
        "        )\n",
        "    else:\n",
        "        X_tr_in, y_tr_in = X_tr, y_tr\n",
        "        X_val, y_val = None, None\n",
        "\n",
        "    # ES: usa callback se disponível (XGB>=2.0); senão, early_stopping_rounds\n",
        "    def _fit_quantile(model, X, y, Xv=None, yv=None):\n",
        "        sig = inspect.signature(model.fit)\n",
        "        kwargs = {}\n",
        "        if Xv is not None and yv is not None and \"eval_set\" in sig.parameters:\n",
        "            kwargs[\"eval_set\"] = [(Xv, yv)]\n",
        "            try:\n",
        "                from xgboost.callback import EarlyStopping as XGB_EarlyStopping\n",
        "                if \"callbacks\" in sig.parameters:\n",
        "                    kwargs[\"callbacks\"] = [XGB_EarlyStopping(rounds=es_rounds, save_best=True, maximize=False)]\n",
        "                elif \"early_stopping_rounds\" in sig.parameters:\n",
        "                    kwargs[\"early_stopping_rounds\"] = es_rounds\n",
        "            except Exception:\n",
        "                if \"early_stopping_rounds\" in sig.parameters:\n",
        "                    kwargs[\"early_stopping_rounds\"] = es_rounds\n",
        "        if \"verbose\" in sig.parameters:\n",
        "            kwargs[\"verbose\"] = False\n",
        "\n",
        "        model.fit(X, y, **kwargs)\n",
        "\n",
        "        best_it = getattr(model, \"best_iteration\", None)\n",
        "        if best_it is None:\n",
        "            try:\n",
        "                best_it = model.get_booster().best_iteration\n",
        "            except Exception:\n",
        "                best_it = None\n",
        "        if best_it is None:\n",
        "            best_it = base_def[\"n_estimators\"]\n",
        "        return int(best_it)\n",
        "\n",
        "    # modelos de quantil low/high\n",
        "    lo = XGBRegressor(objective=\"reg:quantileerror\", quantile_alpha=q_lo, **base_def)\n",
        "    hi = XGBRegressor(objective=\"reg:quantileerror\", quantile_alpha=q_hi, **base_def)\n",
        "\n",
        "    n_lo = _fit_quantile(lo, X_tr_in, y_tr_in, X_val, y_val)\n",
        "    n_hi = _fit_quantile(hi, X_tr_in, y_tr_in, X_val, y_val)\n",
        "\n",
        "    # não-conformidades na calibração\n",
        "    ql_cal = lo.predict(X_cal); qh_cal = hi.predict(X_cal)\n",
        "    ycal = y_cal.values if hasattr(y_cal, \"values\") else np.asarray(y_cal)\n",
        "    scores = np.maximum(ql_cal - ycal, ycal - qh_cal)\n",
        "    qhat = q_conformal(scores, alpha, conservative=CONSERVATIVE_Q)\n",
        "\n",
        "    # refit em train+cal com #árvores selecionadas\n",
        "    X_tc = pd.concat([X_tr, X_cal], axis=0)\n",
        "    y_tc = pd.concat([y_tr, y_cal], axis=0) if hasattr(y_tr, \"values\") else np.concatenate([y_tr, y_cal])\n",
        "\n",
        "    lo2 = XGBRegressor(objective=\"reg:quantileerror\", quantile_alpha=q_lo,\n",
        "                       **{**base_def, \"n_estimators\": n_lo})\n",
        "    hi2 = XGBRegressor(objective=\"reg:quantileerror\", quantile_alpha=q_hi,\n",
        "                       **{**base_def, \"n_estimators\": n_hi})\n",
        "    lo2.fit(X_tc, y_tc); ql_te = lo2.predict(X_te)\n",
        "    hi2.fit(X_tc, y_tc); qh_te = hi2.predict(X_te)\n",
        "\n",
        "    # ponto = q50 ou média dos quantis\n",
        "    if use_q50_point:\n",
        "        mid = XGBRegressor(objective=\"reg:quantileerror\", quantile_alpha=0.5,\n",
        "                           **{**base_def, \"n_estimators\": max(n_lo, n_hi)})\n",
        "        mid.fit(X_tc, y_tc); y_point = mid.predict(X_te)\n",
        "    else:\n",
        "        y_point = 0.5*(ql_te + qh_te)\n",
        "\n",
        "    return y_point, ql_te - qhat, qh_te + qhat, {\"qhat\": float(qhat), \"n_lo\": n_lo, \"n_hi\": n_hi}\n",
        "\n",
        "def pi_enbpi_xgb(\n",
        "    X_tr, y_tr, X_cal, y_cal, X_te,\n",
        "    alpha=ALPHA, seed=SEED,\n",
        "    base_params=None, n_estimators_ens=30, max_samples=0.8\n",
        "):\n",
        "    if not HAVE_XGB:\n",
        "        raise RuntimeError(\"XGBoost não disponível.\")\n",
        "    from xgboost import XGBRegressor\n",
        "\n",
        "    base_def = dict(**XGB_PARAMS_POINT, random_state=seed)\n",
        "    if base_params: base_def.update(base_params)\n",
        "\n",
        "    base = XGBRegressor(objective=\"reg:squarederror\", **base_def)\n",
        "    ensemble = BaggingRegressor(\n",
        "        estimator=base, n_estimators=n_estimators_ens, max_samples=max_samples,\n",
        "        bootstrap=True, n_jobs=-1, random_state=seed\n",
        "    )\n",
        "    ensemble.fit(X_tr, y_tr)\n",
        "    y_cal_pred = ensemble.predict(X_cal)\n",
        "    ycal = y_cal.values if hasattr(y_cal, \"values\") else np.asarray(y_cal)\n",
        "    residuals = np.abs(ycal - y_cal_pred)\n",
        "    q = q_conformal(residuals, alpha, conservative=CONSERVATIVE_Q)\n",
        "    y_te_pred = ensemble.predict(X_te)\n",
        "    return y_te_pred, y_te_pred - q, y_te_pred + q, dict(q_hat=float(q), n_estimators_ens=n_estimators_ens)\n",
        "\n",
        "# =========================\n",
        "# Métricas\n",
        "# =========================\n",
        "def interval_metrics(y_true, y_pred, lower, upper, alpha=ALPHA):\n",
        "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
        "    lower = np.asarray(lower);   upper = np.asarray(upper)\n",
        "    cover = np.mean((y_true >= lower) & (y_true <= upper))\n",
        "    width = np.mean(np.maximum(0, upper - lower))\n",
        "    eta = 10\n",
        "    gamma = 0 if cover >= (1 - alpha) else 1\n",
        "    cwc = width * (1 + gamma * np.exp(-eta * (cover - (1 - alpha))))\n",
        "    return dict(Coverage=float(cover), Width=float(width), CWC=float(cwc),\n",
        "                RMSE=float(rmse(y_true, y_pred)),\n",
        "                MAE=float(mean_absolute_error(y_true, y_pred)),\n",
        "                R2=float(r2_score(y_true, y_pred)))\n",
        "\n",
        "# =========================\n",
        "# Plot\n",
        "# =========================\n",
        "def plot_series(y_true, y_pred, lower, upper, title, outfile):\n",
        "    plt.figure(figsize=(12,5), dpi=150)\n",
        "    idx = np.arange(len(y_true))\n",
        "    plt.plot(idx, y_true, label=\"Real\",    lw=2.0, color=COLOR_REAL)\n",
        "    plt.plot(idx, y_pred, label=\"Predito\", lw=1.8, color=COLOR_PRED)\n",
        "    plt.fill_between(idx, lower, upper, alpha=0.25, color=COLOR_FILL, label=\"Intervalo\")\n",
        "    plt.xlabel(\"Índice (tempo)\")\n",
        "    plt.ylabel(\"SiO₂ in concentrate (%)\")\n",
        "    plt.title(title)\n",
        "    plt.legend(frameon=True)\n",
        "    plt.tight_layout(); plt.grid(False)\n",
        "    plt.savefig(outfile, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "# =========================\n",
        "# Main\n",
        "# =========================\n",
        "METHOD_CHOICES = {\n",
        "    \"t_student\": \"t_student\",\n",
        "    \"full_conformal\": \"full_conformal_cvplus\",\n",
        "    \"full_conformal_f\": \"full_conformal_f\",\n",
        "    \"split_conformal\": \"split_conformal\",\n",
        "    \"bootstrap\": \"bootstrap_residual\",\n",
        "    \"bootstrap_f\": \"bootstrap_f\",\n",
        "    \"cqr_xgb\": \"cqr_xgb\",\n",
        "    \"enbpi_xgb\": \"enbpi_xgb\",\n",
        "}\n",
        "\n",
        "def main():\n",
        "    df = pd.read_csv(CSV_FILE, low_memory=False)\n",
        "    missing = [c for c in FEATURES + [TARGET] if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Colunas faltando no CSV: {missing}\")\n",
        "\n",
        "    if \"Data\" in df.columns:\n",
        "        df[\"Data\"] = pd.to_datetime(df[\"Data\"], errors=\"coerce\")\n",
        "        df = df.sort_values(\"Data\", kind=\"stable\").reset_index(drop=True)\n",
        "\n",
        "    X_full = ensure_numeric(df[FEATURES])\n",
        "    y_full = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
        "\n",
        "    mask_valid = (~X_full.isna().any(axis=1)) & (~y_full.isna())\n",
        "    X_full = X_full.loc[mask_valid].reset_index(drop=True)\n",
        "    y_full = y_full.loc[mask_valid].reset_index(drop=True)\n",
        "\n",
        "    # split primeiro (evita vazamento)\n",
        "    X_tr, X_ca, X_te, y_tr, y_ca, y_te = split_data(X_full, y_full, SPLIT_TYPE, SEED, 0.80, 0.10)\n",
        "\n",
        "    # outliers somente no treino\n",
        "    if USE_ZSCORE_OUTLIERS:\n",
        "        mu, sd = y_tr.mean(), (y_tr.std() if y_tr.std() > 0 else 1.0)\n",
        "        keep_tr = ((y_tr - mu).abs() / sd) <= ZS_THRESH\n",
        "        X_tr = X_tr.loc[keep_tr].reset_index(drop=True)\n",
        "        y_tr = y_tr.loc[keep_tr].reset_index(drop=True)\n",
        "\n",
        "    X_trcal = pd.concat([X_tr, X_ca], axis=0); y_trcal = pd.concat([y_tr, y_ca], axis=0)\n",
        "\n",
        "    interval_model = get_point_model(MODEL_NAME, SEED)\n",
        "    point_model    = get_point_model(MODEL_NAME, SEED)\n",
        "\n",
        "    # ponto separado (opcional)\n",
        "    y_point_te = None\n",
        "    if USE_SEPARATE_POINT_MODEL:\n",
        "        point_model.fit(X_tr, y_tr)\n",
        "        y_point_te = point_model.predict(X_te)\n",
        "\n",
        "    method_key = METHOD_CHOICES.get(METHOD, None)\n",
        "    if method_key is None:\n",
        "        raise ValueError(f\"METHOD inválido: {METHOD}. Opções: {sorted(METHOD_CHOICES.keys())}\")\n",
        "\n",
        "    info = {}\n",
        "    # ================= Dispatch =================\n",
        "    if method_key == \"t_student\":\n",
        "        m = clone(interval_model); m.fit(X_trcal, y_trcal)\n",
        "        ypred = m.predict(X_te)\n",
        "        res_tr = y_trcal.values - m.predict(X_trcal)\n",
        "        lo, up, extra = t_student_intervals(ypred, res_tr, n_train=len(X_trcal), p=X_trcal.shape[1], alpha=ALPHA)\n",
        "        info.update(extra)\n",
        "        y_point = ypred if y_point_te is None else y_point_te\n",
        "        title = \"t-Student (approx.)\"\n",
        "\n",
        "    elif method_key == \"full_conformal_cvplus\":\n",
        "        ypred, lo, up, extra = full_conformal_cvplus(interval_model, X_trcal, y_trcal, X_te, ALPHA, folds=5, seed=SEED)\n",
        "        info.update(extra)\n",
        "        y_point = ypred if y_point_te is None else y_point_te\n",
        "        title = \"Full Conformal (CV+)\"\n",
        "\n",
        "    elif method_key == \"full_conformal_f\":\n",
        "        ypred, lo, up, extra = full_conformal_prediction_intervalsF(interval_model, X_trcal, X_te, y_trcal, alpha=ALPHA)\n",
        "        info.update(extra)\n",
        "        y_point = ypred if y_point_te is None else y_point_te\n",
        "        title = \"Full Conformal\"\n",
        "\n",
        "    elif method_key == \"split_conformal\":\n",
        "        ypred, lo, up, extra = split_conformal_intervals(interval_model, X_tr, y_tr, X_ca, y_ca, X_te, ALPHA)\n",
        "        info.update(extra)\n",
        "        y_point = ypred if y_point_te is None else y_point_te\n",
        "        title = \"Split Conformal\"\n",
        "\n",
        "    elif method_key == \"bootstrap_residual\":\n",
        "      ypred, lo, up, extra = bootstrap_residual_intervals(\n",
        "          interval_model, X_tr, y_tr, X_ca, y_ca, X_te,\n",
        "          n_bootstrap=1000, alpha=ALPHA, seed=SEED\n",
        "      )\n",
        "      info.update(extra)\n",
        "      y_point = ypred if y_point_te is None else y_point_te\n",
        "      title = \"Residual Bootstrap (cal OOS)\"\n",
        "\n",
        "\n",
        "    elif method_key == \"bootstrap_f\":\n",
        "        ypred, lo, up, extra = bootstrap_prediction_intervalsF(interval_model, X_tr, y_tr, X_te, n_bootstrap=1000, alpha=ALPHA, seed=SEED)\n",
        "        info.update(extra)\n",
        "        y_point = ypred if y_point_te is None else y_point_te\n",
        "        title = \"Residual Bootstrap\"\n",
        "\n",
        "\n",
        "    elif method_key == \"ngboost_f\":\n",
        "        ypred, lo, up, extra = pi_ngboostF(X_tr, y_tr, X_te, ALPHA, SEED)\n",
        "        info.update(extra); y_point = ypred if y_point_te is None else y_point_te\n",
        "        title = \"NGBoost (Normal-F)\"\n",
        "\n",
        "    elif method_key == \"cqr_xgb\":\n",
        "        ypred, lo, up, extra = pi_cqr_xgb(\n",
        "            X_tr, y_tr, X_ca, y_ca, X_te, ALPHA, SEED,\n",
        "            xgb_params=XGB_PARAMS_QUANTILE,\n",
        "            use_q50_point=True\n",
        "        )\n",
        "        info.update(extra)\n",
        "        y_point = ypred if y_point_te is None else y_point_te\n",
        "        title = \"CQR (XGB Quantile)\"\n",
        "\n",
        "    elif method_key == \"enbpi_xgb\":\n",
        "        ypred, lo, up, extra = pi_enbpi_xgb(\n",
        "            X_tr, y_tr, X_ca, y_ca, X_te, ALPHA, SEED,\n",
        "            base_params=XGB_PARAMS_POINT\n",
        "        )\n",
        "        info.update(extra)\n",
        "        y_point = ypred if y_point_te is None else y_point_te\n",
        "        title = \"EnbPI (XGB)\"\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"METHOD inválido (dispatch não alcançado).\")\n",
        "\n",
        "    mets = interval_metrics(y_te.values, y_point, lo, up, alpha=ALPHA)\n",
        "\n",
        "    print(\"\\n===== CONFIG =====\")\n",
        "    print(f\"CSV_FILE={CSV_FILE}\")\n",
        "    print(f\"SPLIT_TYPE={SPLIT_TYPE} | MODEL_NAME={MODEL_NAME} | METHOD={METHOD}\")\n",
        "    print(f\"USE_ZSCORE_OUTLIERS={int(USE_ZSCORE_OUTLIERS)} (thr={ZS_THRESH}) | CONSERVATIVE_Q={int(CONSERVATIVE_Q)}\")\n",
        "    print(f\"USE_SEPARATE_POINT_MODEL={int(USE_SEPARATE_POINT_MODEL)}\")\n",
        "    print(\"\\n===== MÉTRICAS =====\")\n",
        "    for k, v in mets.items(): print(f\"{k}: {v:.6f}\")\n",
        "\n",
        "    if info:\n",
        "        print(\"\\n===== INFO DO MÉTODO =====\")\n",
        "        for k, v in info.items():\n",
        "            try: print(f\"{k}: {float(v):.6f}\")\n",
        "            except: print(f\"{k}: {v}\")\n",
        "\n",
        "    out_csv = RESULTS_DIR / f\"series_{METHOD}_{SPLIT_TYPE}_{MODEL_NAME}.csv\"\n",
        "    pd.DataFrame({\"y_true\": y_te.values, \"y_pred\": y_point, \"lower\": lo, \"upper\": up}).to_csv(out_csv, index=False)\n",
        "    print(f\"\\nSéries (teste) salvas em: {out_csv}\")\n",
        "\n",
        "    out_png = RESULTS_DIR / f\"interval_{METHOD}_{SPLIT_TYPE}_{MODEL_NAME}.png\"\n",
        "    plot_series(y_true=y_te.values, y_pred=y_point, lower=lo, upper=up,\n",
        "                title=f\"{title} | α={ALPHA:.2f} | split={SPLIT_TYPE} | model={MODEL_NAME}\",\n",
        "                outfile=str(out_png))\n",
        "    print(f\"Figura salva em: {out_png}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AZCnxtbkifh0"
      },
      "outputs": [],
      "source": [
        "# !DATA_FILE=dados_2h_aggregado_final.csv \\\n",
        "# SPLIT_TYPE=random MODEL_NAME=xgb INTERVAL_METHOD=cqr_xgb ALPHA=0.05 SEED=42 \\\n",
        "# python Ray_flootation4.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iGGnMx1Qd_SJ"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "# run_all_methods_Prediction_intervals.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# =======================\n",
        "# CONFIG\n",
        "# =======================\n",
        "NUM_RUNS    = int(os.environ.get(\"NUM_RUNS\", \"20\"))\n",
        "MAIN_SCRIPT = os.environ.get(\"MAIN_SCRIPT\", \"Prediction_intervals.py\")\n",
        "\n",
        "RESULTS_DIR = Path(\"results\")\n",
        "AGG_DIR     = Path(\"aggregated_results\")\n",
        "AGG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_FILE   = os.environ.get(\"DATA_FILE\", \"dados_2h_aggregado_final.csv\")\n",
        "SPLIT_TYPE  = os.environ.get(\"SPLIT_TYPE\", \"random\")          # random | temporal\n",
        "MODEL_NAME  = os.environ.get(\"MODEL_NAME\", \"xgb\")             # xgb | lgbm\n",
        "ALPHA       = float(os.environ.get(\"ALPHA\", \"0.05\"))\n",
        "\n",
        "# Rode todos (padrão) ou filtre via env ONLY_METHODS=\"enbpi_rf,rf_split,cqr_gbrt\"\n",
        "ALL_METHODS = [\n",
        "    # \"t_student\",\n",
        "    \"full_conformal\",\n",
        "    \"full_conformal_f\",\n",
        "    \"bootstrap_f\",\n",
        "    \"bootstrap\",\n",
        "    \"split_conformal\",\n",
        "    \"cqr_xgb\",\n",
        "    \"enbpi_xgb\",\n",
        "]\n",
        "\n",
        "ONLY = os.environ.get(\"ONLY_METHODS\", \"\").strip()\n",
        "METHODS = [m.strip() for m in ONLY.split(\",\") if m.strip()] if ONLY else ALL_METHODS\n",
        "\n",
        "\n",
        "DISPLAY = {\n",
        "    # \"t_student\":      \"T-Student\",\n",
        "    \"full_conformal_f\":\"Full Conformal\",\n",
        "    # \"full_conformal\": \"Full Conformal CV+\",\n",
        "    \"bootstrap\":      \"Bootstrap\",\n",
        "    \"split_conformal\":\"Split Conformal\",\n",
        "    \"ngboost\":        \"NGBoost\",\n",
        "    \"cqr_xgb\":        \"CQR\",\n",
        "    \"enbpi_xgb\":      \"EnbPI\",\n",
        "}\n",
        "\n",
        "# =======================\n",
        "# Métricas a partir das séries\n",
        "# =======================\n",
        "def rmse(y_true, y_pred):\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    from sklearn.metrics import mean_absolute_error\n",
        "    return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "def r2(y_true, y_pred):\n",
        "    from sklearn.metrics import r2_score\n",
        "    return r2_score(y_true, y_pred)\n",
        "\n",
        "def coverage(y, lo, up):\n",
        "    y = np.asarray(y); lo = np.asarray(lo); up = np.asarray(up)\n",
        "    return float(np.mean((y >= lo) & (y <= up)))\n",
        "\n",
        "def width_mean(lo, up):\n",
        "    lo = np.asarray(lo); up = np.asarray(up)\n",
        "    return float(np.mean(np.maximum(0.0, up - lo)))\n",
        "\n",
        "def winkler_score(y, lo, up, alpha):\n",
        "    \"\"\"Winkler score (quanto menor, melhor).\"\"\"\n",
        "    y = np.asarray(y); lo = np.asarray(lo); up = np.asarray(up)\n",
        "    w = np.maximum(0.0, up - lo)\n",
        "    below = y < lo\n",
        "    above = y > up\n",
        "    pen = np.zeros_like(w)\n",
        "    pen[below] = (2.0 / alpha) * (lo[below] - y[below])\n",
        "    pen[above] = (2.0 / alpha) * (y[above] - up[above])\n",
        "    return float(np.mean(w + pen))\n",
        "\n",
        "def cwc(cover, width, alpha, eta=10.0):\n",
        "    gamma = 0.0 if cover >= (1 - alpha) else 1.0\n",
        "    return float(width * (1 + gamma * math.exp(-eta * (cover - (1 - alpha)))))\n",
        "\n",
        "def metrics_from_series(series_csv, alpha=0.05):\n",
        "    df = pd.read_csv(series_csv)\n",
        "    y   = df[\"y_true\"].values\n",
        "    yph = df[\"y_pred\"].values\n",
        "    lo  = df[\"lower\"].values\n",
        "    up  = df[\"upper\"].values\n",
        "    cov = coverage(y, lo, up)\n",
        "    wid = width_mean(lo, up)\n",
        "    win = winkler_score(y, lo, up, alpha)\n",
        "    return dict(\n",
        "        Coverage=cov,\n",
        "        Width=wid,\n",
        "        Winkler=win,\n",
        "        CWC=cwc(cov, wid, alpha),\n",
        "        RMSE=rmse(y, yph),\n",
        "        MAE=mae(y, yph),\n",
        "        R2=r2(y, yph),\n",
        "    )\n",
        "\n",
        "# =======================\n",
        "# Helpers de plot (pastel)\n",
        "# =======================\n",
        "def pastel_colors(n, cmap_name=\"Pastel1\"):\n",
        "    cmap = cm.get_cmap(cmap_name, max(n, 3))\n",
        "    return [cmap(i) for i in range(n)]\n",
        "\n",
        "def boxplot_metric(df, metric, outpath, target_line=None, ylabel=None, title=None):\n",
        "    methods = df[\"Unnamed: 0\"].unique().tolist()\n",
        "    data = [df.loc[df[\"Unnamed: 0\"] == m, metric].dropna().values for m in methods]\n",
        "    colors = pastel_colors(len(methods), \"Pastel1\")\n",
        "\n",
        "    w = max(8, 1.1*len(methods))\n",
        "    fig, ax = plt.subplots(figsize=(w, 5), dpi=150)\n",
        "    bp = ax.boxplot(data, labels=methods, showfliers=True, patch_artist=True,\n",
        "                    boxprops=dict(linewidth=1.0, edgecolor=\"0.3\"),\n",
        "                    whiskerprops=dict(linewidth=1.0, color=\"0.3\"),\n",
        "                    capprops=dict(linewidth=1.0, color=\"0.3\"),\n",
        "                    medianprops=dict(linewidth=1.2, color=\"black\"))\n",
        "    for patch, c in zip(bp[\"boxes\"], colors):\n",
        "        patch.set_facecolor(c)\n",
        "    if target_line is not None:\n",
        "        ax.axhline(target_line, linestyle=\"--\", linewidth=1.2, color=\"tab:blue\")\n",
        "    ax.set_ylabel(ylabel or metric)\n",
        "    ax.set_title(title or metric)\n",
        "    plt.setp(ax.get_xticklabels(), rotation=20, ha=\"right\")\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.35)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outpath, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(f\"Rodando {NUM_RUNS} execuções de {MAIN_SCRIPT} para métodos: {', '.join(METHODS)}\")\n",
        "    rows = []\n",
        "\n",
        "    for run in range(NUM_RUNS):\n",
        "        print(f\"\\n=== RUN {run+1:02d}/{NUM_RUNS} ===\")\n",
        "        t_run0 = time.time()\n",
        "\n",
        "        for method in METHODS:\n",
        "            env = os.environ.copy()\n",
        "            env[\"DATA_FILE\"]       = DATA_FILE\n",
        "            env[\"SPLIT_TYPE\"]      = SPLIT_TYPE\n",
        "            env[\"MODEL_NAME\"]      = MODEL_NAME\n",
        "            env[\"INTERVAL_METHOD\"] = method\n",
        "            env[\"ALPHA\"]           = str(ALPHA)\n",
        "            env[\"SEED\"]            = str(42 + run)\n",
        "\n",
        "            # mede o tempo do método\n",
        "            t0 = time.time()\n",
        "            proc = subprocess.run([\"python\", MAIN_SCRIPT], capture_output=True, text=True, env=env)\n",
        "            method_dt = time.time() - t0\n",
        "\n",
        "            if proc.returncode != 0:\n",
        "                print(f\"  -> {method}: ERRO — pulando. STDERR:\\n{proc.stderr}\")\n",
        "                continue\n",
        "\n",
        "            # série salva por ray_flotation4.py\n",
        "            series_csv = RESULTS_DIR / f\"series_{method}_{SPLIT_TYPE}_{MODEL_NAME}.csv\"\n",
        "            if not series_csv.exists():\n",
        "                print(f\"  -> {method}: série não encontrada ({series_csv}) — pulando.\")\n",
        "                continue\n",
        "\n",
        "            mets = metrics_from_series(series_csv, alpha=ALPHA)\n",
        "\n",
        "            # linha no formato solicitado (Unnamed: 0 = nome do método)\n",
        "            rows.append({\n",
        "                \"Unnamed: 0\": DISPLAY.get(method, method),\n",
        "                \"Coverage\":   mets[\"Coverage\"],\n",
        "                \"Width\":      mets[\"Width\"],\n",
        "                \"Winkler\":    mets[\"Winkler\"],\n",
        "                \"RMSE\":       mets[\"RMSE\"],\n",
        "                \"R2\":         mets[\"R2\"],\n",
        "                \"MAE\":        mets[\"MAE\"],\n",
        "                \"MethodTime_s\": method_dt,\n",
        "                \"run_id\":     run + 1,\n",
        "                \"execution_time\": np.nan,\n",
        "                \"MethodCode\": method,\n",
        "                \"Split\": SPLIT_TYPE,\n",
        "                \"Model\": MODEL_NAME,\n",
        "            })\n",
        "            print(f\"  -> {DISPLAY.get(method, method)} | cov={mets['Coverage']:.3f} | width={mets['Width']:.3f} | t={method_dt:.2f}s\")\n",
        "\n",
        "        run_dt = time.time() - t_run0\n",
        "        # preenche execução total deste run nas linhas correspondentes\n",
        "        for r in rows:\n",
        "            if np.isnan(r[\"execution_time\"]) and r[\"run_id\"] == (run + 1):\n",
        "                r[\"execution_time\"] = run_dt\n",
        "        print(f\"RUN {run+1:02d} concluído em {run_dt:.2f}s\")\n",
        "\n",
        "    if not rows:\n",
        "        print(\"\\nNenhuma execução bem-sucedida.\")\n",
        "        return\n",
        "\n",
        "    # DataFrame final e salvamento no formato pedido\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # ordena colunas para bater com seu exemplo\n",
        "    ordered_cols = [\n",
        "        \"Unnamed: 0\",\"Coverage\",\"Width\",\"Winkler\",\"RMSE\",\"R2\",\"MAE\",\"MethodTime_s\",\"run_id\",\"execution_time\"\n",
        "    ]\n",
        "    # anexa extras ao final (se existirem)\n",
        "    extra_cols = [c for c in df.columns if c not in ordered_cols]\n",
        "    df = df[ordered_cols + extra_cols]\n",
        "\n",
        "    all_csv = AGG_DIR / \"all_metrics.csv\"\n",
        "    df.to_csv(all_csv, index=False)\n",
        "    print(f\"\\n[OK] salvo: {all_csv}\")\n",
        "\n",
        "    # =======================\n",
        "    # Estatísticas agregadas\n",
        "    # =======================\n",
        "    agg = (df.groupby(\"Unnamed: 0\")\n",
        "             .agg({\n",
        "                 \"Coverage\": [\"mean\",\"std\",\"min\",\"max\"],\n",
        "                 \"Width\":    [\"mean\",\"std\",\"min\",\"max\"],\n",
        "                 \"Winkler\":  [\"mean\",\"std\",\"min\",\"max\"],\n",
        "                 \"RMSE\":     [\"mean\",\"std\",\"min\",\"max\"],\n",
        "                 \"R2\":       [\"mean\",\"std\",\"min\",\"max\"],\n",
        "                 \"MAE\":      [\"mean\",\"std\",\"min\",\"max\"],\n",
        "                 \"MethodTime_s\": \"mean\",\n",
        "                 \"execution_time\": \"mean\"\n",
        "             })\n",
        "             .round(4))\n",
        "    agg.columns = [\"_\".join(c) if isinstance(c, tuple) else c for c in agg.columns]\n",
        "    agg_csv = AGG_DIR / \"aggregated_stats.csv\"\n",
        "    agg.to_csv(agg_csv)\n",
        "    print(f\"[OK] estatísticas: {agg_csv}\")\n",
        "\n",
        "    # porcentagem de execuções abaixo do alvo de cobertura\n",
        "    below = (df.assign(below=lambda d: (d[\"Coverage\"] < (1-ALPHA)).astype(float))\n",
        "               .groupby(\"Unnamed: 0\")[\"below\"].mean() * 100).round(1)\n",
        "    below.to_csv(AGG_DIR / \"coverage_below_target_pct.csv\")\n",
        "    print(\"\\n% de execuções com cobertura < alvo (por método):\")\n",
        "    print(below)\n",
        "\n",
        "\n",
        "    boxplot_metric(df, \"Coverage\", AGG_DIR / \"boxplot_coverage.png\",\n",
        "                   target_line=(1-ALPHA), ylabel=\"Coverage\", title=\"Cobertura por Método\")\n",
        "    boxplot_metric(df, \"Width\",    AGG_DIR / \"boxplot_width.png\",\n",
        "                   ylabel=\"Width\", title=\"Largura do Intervalo por Método\")\n",
        "    boxplot_metric(df, \"Winkler\",  AGG_DIR / \"boxplot_winkler.png\",\n",
        "                   ylabel=\"Winkler\", title=\"Winkler Score por Método\")\n",
        "    boxplot_metric(df, \"RMSE\",     AGG_DIR / \"boxplot_rmse.png\",\n",
        "                   ylabel=\"RMSE\", title=\"RMSE por Método\")\n",
        "    boxplot_metric(df, \"MAE\",      AGG_DIR / \"boxplot_mae.png\",\n",
        "                   ylabel=\"MAE\", title=\"MAE por Método\")\n",
        "\n",
        "\n",
        "    print(\"\\n[OK] Boxplots salvos em:\", AGG_DIR)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
